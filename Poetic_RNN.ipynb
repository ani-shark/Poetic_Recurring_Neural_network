{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#importing libraries and data\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "rKy49nYnF7cz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "metadata": {
        "id": "ircJJpnuLCuT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgJmXG9bLj73",
        "outputId": "48a4022a-67f2-402e-d096-5e79a5672015"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(filepath, 'rb').read().decode(encoding = 'utf-8').lower() #read text file into a variable"
      ],
      "metadata": {
        "id": "M4iOBfi4OwIp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text[300000:800000] #setting a training set"
      ],
      "metadata": {
        "id": "JLn8dSKhPDG0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(set(text)) #get sorted list of unique characters in the text file"
      ],
      "metadata": {
        "id": "GtjfjePLPl0p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c,i) for i,c in enumerate(characters)) #char as key, number as value"
      ],
      "metadata": {
        "id": "AA02SnrpSLHX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char = dict((i,c) for i,c in enumerate(characters)) #number as key, char as value"
      ],
      "metadata": {
        "id": "Yei_oUSiSxPa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 40\n",
        "STEP_SIZE = 3"
      ],
      "metadata": {
        "id": "C28aBUbkT-5G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [] #features\n",
        "next_characters = [] #target"
      ],
      "metadata": {
        "id": "-dx22w2YTK8O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To retrive sentences from the text file into training data\n",
        "for i in range (0,len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "  sentences.append(text[i : i + SEQ_LENGTH])\n",
        "  next_characters.append(text[i + SEQ_LENGTH])"
      ],
      "metadata": {
        "id": "fTXL_Tymx6w-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to boolean data for NN model\n",
        "x = np.zeros((len(sentences), SEQ_LENGTH, len(characters)), dtype=bool)\n",
        "y = np.zeros((len(sentences),len(characters)), dtype=bool)"
      ],
      "metadata": {
        "id": "viyBXY9zyi-N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(sentences):\n",
        "  for t, character in enumerate(sentence):\n",
        "    x[i, t, char_to_index[character]] = 1\n",
        "  y[i, char_to_index[next_characters[i]]] = 1"
      ],
      "metadata": {
        "id": "R02YaGe20Cjz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the NN Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape = (SEQ_LENGTH, len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = RMSprop(lr=0.01))\n",
        "model.fit(x, y, batch_size = 256, epochs = 4)\n",
        "model.save('textgenerator.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdx3Ex4f1ohD",
        "outputId": "48be25d0-b990-41fc-b2df-65157294702b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "651/651 [==============================] - 118s 178ms/step - loss: 2.7135\n",
            "Epoch 2/4\n",
            "651/651 [==============================] - 118s 181ms/step - loss: 2.3138\n",
            "Epoch 3/4\n",
            "651/651 [==============================] - 114s 175ms/step - loss: 2.2010\n",
            "Epoch 4/4\n",
            "651/651 [==============================] - 116s 178ms/step - loss: 2.1231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = tf.keras.models.load_model('textgenerator.model')"
      ],
      "metadata": {
        "id": "I6IZ8EWt49IK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that takes probability predictions from model and chooses one option.Taken from the official keras tutorial\n",
        "def sample(preds, temperature = 1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature # temperature mandates the probability predictions. Higher temperature -> more creativity\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "metadata": {
        "id": "XBYxjvGT5NSC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Starting text from shaksphere (first 40 characters) The text that will be generated after this will be the models creation\n",
        "def generate_text(length, temperature):\n",
        "  start_index = random.randint(0, len(text)- SEQ_LENGTH - 1)\n",
        "  generated = ''\n",
        "  start_sentence = text[start_index : start_index + SEQ_LENGTH]\n",
        "  generated += start_sentence\n",
        "  for i in range(length):\n",
        "    x = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "    for t, character in enumerate(start_sentence):\n",
        "      x[0, t, char_to_index[character]] = 1\n",
        "\n",
        "    predictions = model.predict(x, verbose = 0)[0]\n",
        "    next_index = sample(predictions, temperature)\n",
        "    next_character = index_to_char[next_index]\n",
        "\n",
        "    generated += next_character\n",
        "    start_sentence = start_sentence[1:] + next_character\n",
        "\n",
        "  return generated"
      ],
      "metadata": {
        "id": "Nw4x4VD78EUs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, we try outputting some sentences with varying levels of temperature. temperature is proportional to creativity."
      ],
      "metadata": {
        "id": "A06K_88M-SF8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('- - - - - 0.2 - - - - -')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cRbOCLW_p2_",
        "outputId": "765f71ac-a6b5-4790-e2de-4538d661d498"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- - - - - 0.2 - - - - -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#temperature = 0.2\n",
        "print (generate_text(300, 0.2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn4o2b3I-m8A",
        "outputId": "e0641c44-a9b9-4c60-fb06-aa4358e04200"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s, set upon our foes\n",
            "our ancient word of the her sore the with the hard the sure the sord the the the hard the king and and the mart the mert and my the lord the proust the will be the with the sour the here the will the ward the will the the sore sors mand be the will the the with the hard the beat the mand the cores and and but me the s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('- - - - - 0.4 - - - - -')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znmGES8e_yVE",
        "outputId": "065c7427-a5a4-4c8f-d0d7-e648ed5bbc35"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- - - - - 0.4 - - - - -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#temperature = 0.4\n",
        "print (generate_text(300, 0.4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9XwG3zH_LWq",
        "outputId": "24aed674-e9c6-4352-d27e-e3bc8a02478a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iege,\n",
            "pardon me, if you please; if not, my dond reans my the ring of our mand with me maren shar word in shand her an and sthe for the merser hat me sore the beclond man the hat the sin thou wall the wist be to that and reat of searn the pronger with and and the me that with the the cond her proment richord for the self il the freck no me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('- - - - - 0.6 - - - - -')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbFOMzow_0hW",
        "outputId": "964c336f-48fc-4329-c7ef-94a184a6ba93"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- - - - - 0.6 - - - - -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#temperature = 0.6\n",
        "print (generate_text(300, 0.6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCYmHfWh_LgX",
        "outputId": "9a968d4a-5f8f-420b-b4fe-6d2f9ebf20a3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t dropp'd down yet.\n",
            "\n",
            "first lord:\n",
            "the highe fard smer sor this sorin\n",
            "\n",
            "apricking the sing mum and and watherfire theren thy mave\n",
            "romy manc:\n",
            "in of the mend to comest bet mist the king the her sann me lpateru? and son y more's mingen and in shis thes bland beny stall the romling to the to the ou menger mand and hart shen in ant un the kence g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('- - - - - 0.8 - - - - -')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFnUB3i7_2sk",
        "outputId": "fc4352ab-72ee-47eb-bff8-06b4a7408854"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- - - - - 0.8 - - - - -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#temperature = 0.8\n",
        "print (generate_text(300, 0.8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TIr3jur_Lta",
        "outputId": "05b46e06-afaf-44f7-bc24-ff58043bd724"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adversary,\n",
            "and yours, close fighting ered this sangr-ingor be,\n",
            "and ix comenstanct now:\n",
            "to noy armay\n",
            "the butd be the rintt liomn's not.\n",
            "\n",
            "lavesol foll it not anle whill have rence than woots nontor ale thall switl oul lenges mingiry,\n",
            "list lang and thaug nom frmarde the bu y the nortetor'd in me ty ur tous ond;\n",
            "in stert for my mingrangy and \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('- - - - - 1.0 - - - - -')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwTRaxr2_5S4",
        "outputId": "849cd068-7d82-492d-eb52-be4a55ca8dfc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- - - - - 1.0 - - - - -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#temperature = 1.0\n",
        "print (generate_text(300, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUZxOOLH_L4D",
        "outputId": "ad07385c-9f19-48e9-b339-d2b3d212aa71"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vered with a groan,\n",
            "'o, farewell, warwick, ay plisitt and,\n",
            "nor glloin ferinnt stlluse broge ed, the tient.\n",
            "\n",
            "heroon\n",
            "on sive;\n",
            "ind't sherfore?\n",
            "\n",
            "pprcadne,\n",
            "ma may herse;\n",
            "ond ofst and that\n",
            "mofr wunt nosh lo, cemackint mo ant gor enrthe,\n",
            "and cooc stay,\n",
            "a parime.\n",
            "\n",
            "guruek:\n",
            "th sicllive ane so to bes,\n",
            "tay grey?\n",
            "\n",
            "for pmy,\n",
            "yor ih may wailn,\n",
            "to i gone!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (generate_text(300, 0.29))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWM4NDoMCZe8",
        "outputId": "31acffdf-4bde-4cc3-bc69-e1324dac7c51"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ng the principal of all the deer.\n",
            "\n",
            "secone:\n",
            "i hard me praint:\n",
            "and and will the but the soust thou sard the his me the greand and my but the ward in the here with the his the pringer beand but the sour and the promes and my prave the the mand and shard and the sore the king the mand that me hard are and and and and the mand and the ronge fr\n"
          ]
        }
      ]
    }
  ]
}